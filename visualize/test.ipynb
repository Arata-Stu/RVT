{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python-based detection evaluation\n",
      "Set MaxViTRNN backbone (height, width) to (512, 640)\n",
      "Set partition sizes: (8, 10)\n",
      "Set num_classes=3 for detection head\n",
      "inchannels: (64, 128, 256)\n",
      "strides: (8, 16, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from config.modifier import dynamically_modify_train_config\n",
    "from modules.utils.fetch import fetch_data_module, fetch_model_module\n",
    "from models.detection.yolox.utils.boxes import postprocess\n",
    "\n",
    "yaml_path = './gen4_dt_20.yaml'\n",
    "config = OmegaConf.load(yaml_path)\n",
    "dynamically_modify_train_config(config)\n",
    "\n",
    "data = fetch_data_module(config)\n",
    "model = fetch_model_module(config)\n",
    "model.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import torch\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# from modules.utils.detection import RNNStates\n",
    "# # 動画のパス\n",
    "# video_path = \"output.avi\"\n",
    "\n",
    "# # 動画を開く\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# # 画像をTensorに変換するためのTransform定義\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),  # NumPy画像をPIL画像に変換\n",
    "#     transforms.Resize((384, 640)),  # 画像サイズをリサイズ\n",
    "#     transforms.ToTensor(),  # Tensorに変換\n",
    "# ])\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.eval()  # 推論モード\n",
    "# model.to(device)  # モデルをGPUに転送\n",
    "\n",
    "# ckpt_path = config.ckpt_path\n",
    "# if ckpt_path != \"\":\n",
    "#     print(f\"Loading checkpoint from {ckpt_path}, device: {device}\")\n",
    "#     ckpt = torch.load(ckpt_path, map_location=device)  # デバイスに合わせてチェックポイントをロード\n",
    "#     model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "# rnn_state = RNNStates()\n",
    "# rnn_state.reset(worker_id=0, indices_or_bool_tensor=True)\n",
    "# prev_states = rnn_state.get_states(worker_id=0)\n",
    "\n",
    "# # フレームごとに処理\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()  # フレームを取得\n",
    "#     if not ret:\n",
    "#         break  # 動画が終了したらループを抜ける\n",
    "\n",
    "#     # BGR(OpenCV) → RGB\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # NumPy → Tensor\n",
    "#     frame_tensor = transform(frame)  # shape: (3, 224, 224)\n",
    "#     frame_tensor = frame_tensor.unsqueeze(0)  # shape: (1, 3, 224, 224) バッチ次元追加\n",
    "#     frame_tensor = frame_tensor.to(device)  # テンソルをGPUに転送\n",
    "\n",
    "#     # モデルに入力（推論実行）\n",
    "#     with torch.no_grad():\n",
    "#         backbone_features, states = model.mdl.forward_backbone(x=frame_tensor, previous_states=prev_states)\n",
    "#         prev_states = states\n",
    "#         rnn_state.save_states_and_detach(worker_id=0, states=prev_states)\n",
    "\n",
    "#         predictions, _ = model.mdl.forward_detect(backbone_features=backbone_features)\n",
    "#         pred_processed = postprocess(prediction=predictions, num_classes=3, conf_thre=0.1, nms_thre=0.45)\n",
    "\n",
    "#     # 結果を表示（ダミー出力の形状を確認）\n",
    "#     print(pred_processed)  # (1, 16, 224, 224)\n",
    "\n",
    "# # 動画を閉じる\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./ckpt/gen4_20.ckpt, device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arata22/project/event_camera/RVT/rvt/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "from modules.utils.detection import RNNStates\n",
    "from models.detection.yolox.utils.boxes import postprocess\n",
    "\n",
    "class VideoVisualizer:\n",
    "    def __init__(self, output_path, fps=30):\n",
    "        self.video_writer = None\n",
    "        self.output_path = output_path\n",
    "        self.fps = fps\n",
    "        self.mode = 4  # すべての可視化を行う\n",
    "    \n",
    "    def create_video_writer(self, frame_shape):\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        self.video_writer = cv2.VideoWriter(self.output_path, fourcc, self.fps, frame_shape)\n",
    "\n",
    "    def visualize(self, ev_tensor, labels_yolox, predictions):\n",
    "        ev_tensor = ev_tensor.squeeze(0).detach().cpu().numpy().astype('uint8').copy()\n",
    "\n",
    "        if ev_tensor.shape[0] == 3:\n",
    "            ev_tensor = np.transpose(ev_tensor, (1, 2, 0))  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "        ev_tensor = (ev_tensor * 255).clip(0, 255).astype('uint8')\n",
    "\n",
    "\n",
    "        if self.video_writer is None:\n",
    "            self.create_video_writer(frame_shape=(ev_tensor.shape[1], ev_tensor.shape[0]))\n",
    "\n",
    "        # 指定色を置換する\n",
    "        # red_mask = (ev_tensor[:, :, 0] == 255) & (ev_tensor[:, :, 1] == 0) & (ev_tensor[:, :, 2] == 0)\n",
    "        # ev_tensor[red_mask] = [255, 255, 255]\n",
    "\n",
    "        # blue_mask = (ev_tensor[:, :, 0] == 0) & (ev_tensor[:, :, 1] == 0) & (ev_tensor[:, :, 2] == 255)\n",
    "        # ev_tensor[blue_mask] = [0, 0, 0]\n",
    "\n",
    "        \n",
    "\n",
    "        # RGB -> BGR変換\n",
    "        ev_tensor = cv2.cvtColor(ev_tensor, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        cv2.imshow(\"Debug Frame\", ev_tensor)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "        # YOLOXのラベル描画\n",
    "        if self.mode in [2, 4] and labels_yolox is not None:\n",
    "            for cls, cx, cy, w, h in labels_yolox[0]:\n",
    "                if any(val is None or np.isnan(val) for val in [cx, cy, w, h]):\n",
    "                    continue\n",
    "                x = max(0, int(cx - w / 2))\n",
    "                y = max(0, int(cy - h / 2))\n",
    "                x2 = min(ev_tensor.shape[1] - 1, int(cx + w / 2))\n",
    "                y2 = min(ev_tensor.shape[0] - 1, int(cy + h / 2))\n",
    "                color = (0, 255, 0)  # 緑色\n",
    "                cv2.rectangle(ev_tensor, (x, y), (x2, y2), color, 2)\n",
    "                label = f\"{cls}\"\n",
    "                cv2.putText(ev_tensor, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "        # 予測結果の描画\n",
    "        if self.mode in [3, 4] and predictions is not None and predictions[0] is not None:\n",
    "            for x1, y1, x2, y2, obj_conf, class_conf, class_id in predictions[0]:\n",
    "                x1 = max(0, int(x1))\n",
    "                y1 = max(0, int(y1))\n",
    "                x2 = min(ev_tensor.shape[1] - 1, int(x2))\n",
    "                y2 = min(ev_tensor.shape[0] - 1, int(y2))\n",
    "                color = (0, 255, 255)  # 黄色\n",
    "                cv2.rectangle(ev_tensor, (x1, y1), (x2, y2), color, 2)\n",
    "                label = f\"{class_id:.2f}\"\n",
    "                cv2.putText(ev_tensor, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "        # フレームを書き込み\n",
    "        self.video_writer.write(ev_tensor)\n",
    "\n",
    "    def close(self):\n",
    "        if self.video_writer is not None:\n",
    "            self.video_writer.release()\n",
    "\n",
    "# 動画パス\n",
    "video_path = \"output_100.avi\"\n",
    "output_path = \"output_visualized_100.avi\"\n",
    "\n",
    "# 動画を開く\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# 画像をTensorに変換するためのTransform定義\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((512, 640)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "ckpt_path = config.ckpt_path\n",
    "if ckpt_path != \"\":\n",
    "    print(f\"Loading checkpoint from {ckpt_path}, device: {device}\")\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "rnn_state = RNNStates()\n",
    "rnn_state.reset(worker_id=0, indices_or_bool_tensor=True)\n",
    "prev_states = rnn_state.get_states(worker_id=0)\n",
    "\n",
    "visualizer = VideoVisualizer(output_path=output_path, fps=30)\n",
    "\n",
    "# フレームごとに処理\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_tensor = transform(frame)\n",
    "    frame_tensor = frame_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    # モデル推論\n",
    "    with torch.no_grad():\n",
    "        backbone_features, states = model.mdl.forward_backbone(x=frame_tensor, previous_states=prev_states)\n",
    "        prev_states = states\n",
    "        rnn_state.save_states_and_detach(worker_id=0, states=prev_states)\n",
    "\n",
    "        predictions, _ = model.mdl.forward_detect(backbone_features=backbone_features)\n",
    "        pred_processed = postprocess(prediction=predictions, num_classes=3, conf_thre=0.1, nms_thre=0.45)\n",
    "\n",
    "    # 可視化\n",
    "    visualizer.visualize(frame_tensor, labels_yolox=None, predictions=pred_processed)\n",
    "\n",
    "cap.release()\n",
    "visualizer.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
